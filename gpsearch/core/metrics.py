import numpy as np
from .utils import custom_KDE, compute_mean, compute_mean_jac
from .minimizers import funmin


def mll(m_list, inputs, pts=None, yy=None, accumulate=False):
    """Mean log loss as defined in (23) of Merchant and Ramos, ICRA 2014.

    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    pts : array_like
        Sampled points used for RMSE computation.
    yy : array_like
        Output values of the true map at `pts`.
    accumulate : boolean, optional
        Whether or not to report the running minimum of the metric.
       
    Returns
    -------
    res : list
        A list containing the values of the MLL for each model 
        in `m_list`. 

    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        mu, var = model.predict(pts)
        mu, var, yy = mu.flatten(), var.flatten(), yy.flatten() 
        res[ii] = 0.5 * np.mean( np.log(2*np.pi*var) + (mu-yy)**2/var )

    if accumulate:
        res = np.minimum.accumulate(res)

    return res


def rmse(m_list, inputs, pts=None, yy=None, accumulate=False):
    """Root-mean-square error between GP model and objective function.

    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    pts : array_like
        Sampled points used for RMSE computation.
    yy : array_like
        Output values of the true map at `pts`.
    accumulate : boolean, optional
        Whether or not to report the running minimum of the metric.
       
    Returns
    -------
    res : list
        A list containing the values of the RMSE for each model 
        in `m_list`. 

    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        mu = model.predict(pts)[0]
        diff = mu.flatten() - yy.flatten()
        res[ii] = np.sqrt(np.mean(np.square(diff)))
    if accumulate:
        res = np.minimum.accumulate(res)
    return res
    

def log_pdf(m_list, inputs, pt=None, pts=None, clip=True, 
            accumulate=False):
    """Log-error between estimated pdf and true pdf.

    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    pt : instance of `FFTKDE` or dummy class
        The true pdf. If a dummy class is passed, it needs to have
        a method called `evaluate`. Dummy class is useful when the 
        true pdf has an analytical form and there is no need for KDE.
    pts : array_like
        Randomly sampled points used for KDE of the GP model.
    clip : boolean, optional
        Whether or not to clip the pdf values below machine-precision.
    accumulate : boolean, optional
        Whether or not to report the running minimum of the metric.
       
    Returns
    -------
    res : list
        A list containing the values of the log-error for each model 
        in `m_list`. The log-error is defined as
            e = \int | log(pdf_{GP}) - log(pdf_{true}) | dy 

    """

    res = np.zeros(len(m_list))

    for ii, model in enumerate(m_list):

        mu = model.predict(pts)[0].flatten()
        ww = inputs.pdf(pts)
        pb = custom_KDE(mu, weights=ww)

        x_min = min( pb.data.min(), pt.data.min() )
        x_max = max( pb.data.max(), pt.data.max() )
        rang = x_max-x_min
        x_eva = np.linspace(x_min - 0.01*rang,
                            x_max + 0.01*rang, 1024)

        yb, yt = pb.evaluate(x_eva), pt.evaluate(x_eva)
        log_yb, log_yt = np.log(yb), np.log(yt)

        if clip: # Clip to machine-precision
            np.clip(log_yb, -14, None, out=log_yb)
            np.clip(log_yt, -14, None, out=log_yt)

        log_diff = np.abs(log_yb-log_yt)
        noInf = np.isfinite(log_diff)
        res[ii] = np.trapz(log_diff[noInf], x_eva[noInf])

    if accumulate:
        res = np.minimum.accumulate(res)

    return res


def regret_tmap(m_list, inputs, true_ymin=0, tmap=None, 
                accumulate=False):
    """Immediate regret using objective function.

    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    true_ymin : float, optional
        The minimum value of the objective function.
    tmap : instance of `BlackBox`
        The black box.
    accumulate : boolean, optional
        Whether or not to report the running minimum of the metric.
       
    Returns
    -------
    res : list
        A list containing the values of the immediate regret for each 
        model in `m_list` using the black-box objective function:
            $r(n) = f(x_n) - y_{true}$
        where f is the black box, x_n the algorithm recommendation at 
        iteration n, and y_{true} the minimum of the objective function.

    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        x_min = recommend(model, inputs)
        y_min = tmap.evaluate(x_min, include_noise=False)
        res[ii] = y_min - true_ymin
    if accumulate:
        res = np.minimum.accumulate(res)
    return res


def regret_model(m_list, inputs, true_ymin=0, accumulate=False):
    """Immediate regret using surrogate GP model.
    
    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    true_ymin : float, optional
        The minimum value of the objective function.
    accumulate : boolean, optional
        Whether or not to report the running minimum of the metric.
       
    Returns
    -------
    res : list
        A list containing the values of the immediate regret for each 
        model in `m_list` using the surrogate GP model:
            $r(n) = \hat{f}_n(x_n) - y_{true}$
        where \hat{f}_n is the GP model at iteration n, x_n the 
        algorithm recommendation at iteration n, and y_{true} the 
        minimum of the objective function.

    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        x_min = recommend(model, inputs)
        y_min = compute_mean(x_min, model)
        res[ii] = np.abs(y_min - true_ymin)
    if accumulate:
        res = np.minimum.accumulate(res)
    return res


def regret_obs(m_list, inputs, true_ymin=0):
    """Immediate regret using past observations.
    
    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    true_ymin : float, optional
        The minimum value of the objective function.
       
    Returns
    -------
    res : list
        A list containing the values of the immediate regret for each 
        model in `m_list` using past observations:
            $r(n) = min y_i - y_{true}$
        where y_i are the observations recorded in the first `n` 
        iterations, and y_{true} the minimum of the objective function. 

    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        res[ii] = model.Y.min() - true_ymin
    return res


def distmin_model(m_list, inputs, true_xmin=[], accumulate=False):
    """Distance to minimum using surrogate GP model.
    
    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    true_xmin : array_like or list
        The locations of the minima of the objective function.
    accumulate : boolean, optional
        Whether or not to report the running minimum of the metric.
       
    Returns
    -------
    res : list
        A list containing the values of the distance to minimum for each 
        model in `m_list` using the surrogate GP model:
            $\ell(n) = \Vert x_n - x_{true} \Vert^2$
        where x_n is the algorithm recommendation at iteration n, and 
        x_{true} the location of the minimum of the objective function.
        When more than one global minimum exists, we compute the 
        distance to each minimum and report the smallest value.
    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        x_min = recommend(model, inputs)
        l2_dist = [ np.linalg.norm(x_min - true) for true in true_xmin ]
        res[ii] = min(l2_dist)
    if accumulate:
        res = np.minimum.accumulate(res)
    return res


def distmin_obs(m_list, inputs, true_xmin=[]):
    """Distance to minimum using past observations.
    
    Parameters
    ----------
    m_list : list
        A list of GPy models generated by `OptimalDesign`.
    inputs : instance of `Inputs`
        The input space.
    true_xmin : array_like or list
        The locations of the minima of the objective function.
       
    Returns
    -------
    res : list
        A list containing the values of the distance to minimum for each 
        model in `m_list` using past observations:
            $\ell(n) = \Vert argmin x_i - x_{true} \Vert^2$
        where x_i is the location of the current best observation,
        and x_{true} the location of the minimum of the objective 
        function. When more than one global minimizer exists, we 
        compute the distance to each minimizer and report the smallest 
        value.
    """
    res = np.zeros(len(m_list))
    for ii, model in enumerate(m_list):
        x_min = model.X[np.argmin(model.Y)]
        l2_dist = [ np.linalg.norm(x_min - true) for true in true_xmin ]
        res[ii] = min(l2_dist)
    return res


def recommend(model, inputs, num_restarts=10, parallel_restarts=False):
    """Compute recommendation for where minimum is located.
    
    Parameters
    ----------
    model : instance of `GPRegression`
        A GPy model.
    inputs : instance of `Inputs`
        The input space.
    num_restarts : int, optional
        Number of restarts for the optimizer. 
    parallel_restarts : boolean, optional
        Whether or not to solve the optimization problems in parallel.
       
    Returns
    -------
    x_min : array
        The recommendation for where the GP model believes the global 
        minimum is located.
    """
    if parallel_restarts:
        set_worker_env()
    x_min = funmin(compute_mean,
                   compute_mean_jac,
                   inputs,
                   args=(model,),
                   num_restarts=num_restarts,
                   parallel_restarts=parallel_restarts,
                   init_method="sample_fun")
    return x_min



